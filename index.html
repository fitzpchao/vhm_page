<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VHM</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon1.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div id="title-container">
            
                <!-- Image column -->
                  <!-- <figure class="image" style="max-width: 60%;"> Adjust the width as necessary -->
                    <img src="static/images/h2rsvlm_logo-removebg-preview.png" alt="Description of Image">
                  <!-- </figure> -->
                <!-- Title column -->
                  <h1 class="title is-1 publication-title">
                    VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis
                  </h1>
              
            </div>
            
            <!-- <h1 class="title is-1 publication-title">H<sup>2</sup>RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model</h1> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/fitzpchao" target="_blank">Chao Pang</a><sup>1</sup><sup>*</sup>,</span>
                <span class="author-block">
                  Xingxing Weng<sup>1</sup><sup>*</sup>
                </span>,
                <span class="author-block">
                  <a href="https://github.com/jxd0712" target="_blank">Jiang Wu</a><sup>2</sup><sup>*</sup>,</span>
                  <span class="author-block">
                    Jiayu Li<sup>1</sup>
                  </span>,
                  <span class="author-block">
                    <a href="https://tinaliu123.github.io/" target="_blank">Yi Liu</a><sup>1</sup>
                  </span>,
                  <span class="author-block">
                    Jiaxing Sun<sup>1</sup>
                  </span>,
                  <span class="author-block">
                    <a href="https://liweijia.github.io/" target="_blank">Weijia Li<sup>3</sup></a>
                  </span>,
                  <span class="author-block">
                    Shuai Wang<sup>4</sup>
                  </span>,
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=PnNAAasAAAAJ&hl=en" target="_blank">Litong Feng<sup>4</sup></a>
                  </span>,
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=SAUCVsEAAAAJ&hl=en" target="_blank">Gui-Song Xia<sup>1</sup><sup>&dagger;</sup></a>
                  </span>,
                  <span class="author-block">
                    <a href="https://conghui.github.io/" target="_blank">Conghui He<sup>2,4</sup><sup>&dagger;</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">1. Wuhan University  2. Shanghai AI Lab 3. Sun Yat-Sen University 4. Sensetime Research <br>arXiv 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution,</small><sup>&dagger;</sup>Indicates Corresponding authors</small></span>
                  </div>
                  
                 

                  <div class="column has-text-centered">

                    <div class="publication-links">

                         <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2403.20213" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/opendatalab/VHM" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                    </span>
                        
                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/FitzPC/VHM_dataset_pretrain" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-database"></i>
                        </span>
                        <span>Dataset</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/spaces/FitzPC/VHM" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-robot"></i>
                      </span>
                      <span>Demo</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/FitzPC/vhm_7B" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-download"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>



                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/h2rsvlm_logo.png" class="center-image" style="max-width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <em><strong>Versatile and Honest vision language Model (VHM). Contributions: 1) We constructed VersaD, a large-scale dataset of 1.4 million remote sensing image and caption pairs. 2) We created HnstD, an RS-specific honest dataset comprising questions with factual and deceptive categories 3) We explored numerous specialized tasks for remote sensing and achieved promising results.</strong></em>
        </h2>
      </div>
      <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
        <!-- Your image here -->
        <img src="static/images/Fig_hqdc_construct_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <em><strong>The construction pipeline of VersaD dataset. We utilize images from publicly available remote sensing datasets and design effective prompts to induce Google's commercial Gemini-Vision model to generate captions for remote sensing images.</strong></em>
        </h2>
      </div>
      <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
        <!-- Your image here -->
        <img src="static/images/Fig_VLM_chat_example_00.png" class="center-image" style="max-width: 100%; height: auto; top:auto; bottom: auto; vertical-align: middle;"/>
        <h2 class="subtitle has-text-centered">
          <em><strong>Conversations between user and VHM</strong></em>
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">VersaD for Pretrain</h2>
      <div class="level-set has-text-justified">
        <p>We utilized the <a href="https://ai.google.dev/docs/gemini_api_overview">gemini-1.0-pro-vision </a> API to generate descriptions for images from multiple public RS datasets, thereby obtaining a dataset of image-text pairs to serve as the pre-training data for RSVLMs.</p>
     </div>
     <br><br>
     <figure>
      <img src="static/images/Fig_dataset_compared_00.png" alt="">
      <figcaption>Examples of large-scale RS vision-language datasets. RS5M consists of two subsets: RS5M-RS3 and RS5M-
        PUB11. <strong>(a)</strong> VersaD (ours) captions provide detailed descriptions, including <span class="red">moving objects</span>, <span class="customgreen">rapidly changing information</span> (i.e., the seasonal feature in vegetation) and rich attributes of objects.
        <strong>(b)</strong> RS5M-RS3's captions are short and lack detail.
        <strong>(c)</strong> RS5M-PUB11's images are not typical RS images. The captions are short, repetitive texts, often containing <span class="orange">redundance</span> (such as "two houses" in the figure).
        <strong>(d)</strong> SkyScript captions are from OpenStreetMap, therefore lacking moving objects and seasonal sensitive information; the descriptions are relatively short. (a) and (b) share the same image. We selected images of similar scene for (c) and (d).</figcaption>
    </figure>

    <br><br>
     <figure>
      <img src="static/images/Fig_hqdc_examples_00.png" alt="">
      <figcaption>Examples of VersaD. <strong>Black sentences</strong> represent descriptions that are entirely accurate, <span class="customgreen">green sentences</span> represent descriptions that are partially accurate, and <span class="red">red sentences</span> represent descriptions that are entirely incorrect.</figcaption>
    </figure>
      </div>
   </div>
 </div>
</section>

<section class="section hero is-small is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Datasets for Supervised Fine Tuning</h2>
      <div class="level-set has-text-justified">
        <p>The training instrutions during the Supervised Fine Tuning (SFT) stage comprises four parts, each with a specified quantity: <strong>the VersaD-Instruct dataset (30k)</strong>, <strong>the HnstD  dataset (44k)</strong>, <strong>the RS-Specialized-Instruct dataset (29.8k)</strong>, and <strong>the RS-ClsQaGrd-Instruct dataset (78k)</strong>, summing up to a total of <strong>180k</strong></p>
     </div>
     <div id="results-carousel" class="carousel results-carousel" style="display: flex; justify-content: center;">
     <!-- <div id="results-carousel" class="carousel results-carousel" display: flex; justify-content: center;> -->
      <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
       <!-- Your image here -->
       <img src="static/images/Fig_hqdc_instruct_00.png" class="center-image" style="max-width: 100%; height: auto;" />
       <h2 class="subtitle has-text-centered">
        Examples of VersaD-Instruct Dataset. 
       </h2>
     </div>

     <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
       <!-- Your image here -->
       <img src="static/images/Fig_dataset-RSSA_examples_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
       <h2 class="subtitle has-text-centered">
        Sample examples in HnstD dataset.
       </h2>
     </div>
     <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/rs_specialized.jpg" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Details of RS-Specialized-Instruct Dataset and Performance of VHM.
      </h2>
    </div>
    <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/rs_clsqaGrd.jpg" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Detailed introduction of the RS-ClsQaGrd-Instruct datasets.
      </h2>
    </div>
 </div>
    </div>
   </div>
 </div>
</section>



<section class="section hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">VHM Implement</h2>
      <div class="level-set has-text-justified">
        <p>We adopted the LLaVA model and continued to train it to obtain the VHM. It includes three main components:

          (1) A pretrained vision encoder using the CLIP-Large model, with a resolution of 336 &times; 336 and a patch size of 14, capable of converting input images into 576 tokens.
          
          (2) An LLM based on the open-source Vicuna-v1.5, originating from LLaMA2. We use the 7B-version throughout this paper.
          
          (3) It incorporates a projector, which is a multilayer perceptron composed of two layers, used to connect the vision encoder and the LLM.</p>
     </div>
     <br><br>
     <figure>
      <img src="static/images/Fig_VLM_train_stage_00.png" alt="">
      <figcaption>Our training is divided into two stages: the first stage is pretraining, and the second stage is supervised fine-tuning.</figcaption>
    </figure>
      </div>
   </div>
 </div>
</section>


<section class="section hero is-small is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3"> RS Specialized Function of VHM </h2>
     <div id="results-carousel" class="carousel results-carousel">
      <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
       <!-- Your image here -->
       <img src="static/images/Fig_gsd_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
       <h2 class="subtitle has-text-centered">
        Examples of the results from image ground sampling distance estimation task.
       </h2>
     </div>

     <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
       <!-- Your image here -->
       <img src="static/images/Fig_imgtype_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
       <h2 class="subtitle has-text-centered">
        Examples of the results from image type identification task.
       </h2>
     </div>
     <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/Fig_counting_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Examples of the results from counting task.
      </h2>
    </div>
    <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/Fig_vg_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Examples of the results from visual grounding task.The red box represents the predicted result, while the blue box represents the ground truth.
      </h2>
    </div>
    <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/Fig_polygon_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Examples of the results from building footprint vectorization task. The red polygon represents the predicted result, while the blue polygon represents the ground truth.

      </h2>
    </div>
    <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/Fig_mcls_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Examples of the results from multi-label landcover classification task.
      </h2>
    </div>
    <div class="item" style=" height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center;">
      <!-- Your image here -->
      <img src="static/images/Fig_objmeas_00.png" class="center-image" style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Examples of the results from object measurement task.
      </h2>
    </div>
 </div>
    </div>
   </div>
 </div>
</section>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{pang2024h2rsvlm,
        title={H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model}, 
        author={Chao Pang and Jiang Wu and Jiayu Li and Yi Liu and Jiaxing Sun and Weijia Li and Xingxing Weng and Shuai Wang and Litong Feng and Gui-Song Xia and Conghui He},
        year={2024},
        eprint={2403.20213},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <h2 class="title">Contact</h2>
        <!-- <div class="TEXT"> -->
          <p>
            If you have any problems or feedback with the Project, please contact:
          </p>
          <ul style="list-style-type: disc;">
            <li>Chao Pang at <strong>pangchao@whu.edu.cn</strong></li>
            <li>Xingxing Weng at <strong>wengxingxing@whu.edu.cn</strong></li>
            <li>Jiang Wu at <strong>wujiang@pjlab.org.cn</strong></li>
            <li>Gui-Song Xia at <strong>guisong.xia@whu.edu.cn</strong></li>
            <li>Conghui He at <strong>heconghui@pjlab.org.cn</strong></li>
          </ul> 
        <!-- </div> -->
      </div>
    </div>
  </div>
</footer>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
